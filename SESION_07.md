# Sesi√≥n 7: Verificaci√≥n de Supuestos y Determinaci√≥n de Correlaciones

*(Pre-requisito: Aseg√∫rese de tener el paquete `readr` y `ggplot2` instalados, y el archivo `datos_ventas.csv` en su carpeta de proyecto).*

## 7.1 Verificaci√≥n de Supuestos (Normalidad)

### üí° Explicaci√≥n del Concepto

Muchos modelos estad√≠sticos (como la Regresi√≥n Lineal que veremos en la Sesi√≥n 8) *asumen* que los datos, o m√°s precisamente los "residuos" (errores) del modelo, siguen una **distribuci√≥n normal** (la "campana de Gauss").

Si los datos no son normales, el modelo puede dar resultados err√≥neos o poco fiables. Por lo tanto, debemos verificar este supuesto. Hay dos formas principales de hacerlo:

1.  **Inspecci√≥n Visual (La m√°s recomendada):**
    * **Histograma/Gr√°fico de Densidad:** (Visto en Sesi√≥n 6). ¬øEl gr√°fico parece una campana sim√©trica?
    * **Gr√°fico Q-Q (Cuantil-Cuantil):** Este es el mejor gr√°fico para esta tarea. Compara los cuantiles de nuestros datos (eje Y) contra los cuantiles de una distribuci√≥n normal te√≥rica perfecta (eje X).
        * **Interpretaci√≥n:** Si los puntos de datos siguen *perfectamente* la l√≠nea diagonal, los datos son perfectamente normales. Peque√±as desviaciones en las "colas" (extremos) son comunes y aceptables. Grandes desviaciones (ej. una forma de "S" o "banana") indican que los datos no son normales.

2.  **Prueba Estad√≠stica (Test de Hip√≥tesis):**
    * **Test de Shapiro-Wilk (`shapiro.test()`):** Es una prueba formal muy popular para muestras peque√±as (t√≠picamente < 5000 registros).
    * **Hip√≥tesis Nula ($H_0$):** "Los datos S√ç provienen de una distribuci√≥n normal".
    * **Hip√≥tesis Alternativa ($H_1$):** "Los datos NO provienen de una distribuci√≥n normal".
    * **Interpretaci√≥n (Valor *p*):**
        * Si **p-valor > 0.05**: *No podemos rechazar* $H_0$. Asumimos que los datos son "suficientemente normales". (¬°Resultado deseado!)
        * Si **p-valor < 0.05**: *Rechazamos* $H_0$. Los datos **no** se distribuyen normalmente.
    * *Precauci√≥n:* En datasets muy grandes, esta prueba es demasiado "sensible" y casi siempre rechazar√° $H_0$ (p-valor < 0.05), incluso si los datos son visualmente "suficientemente normales". Por eso, la inspecci√≥n visual (Q-Q Plot) suele ser m√°s pr√°ctica.

### üíª Ejemplos de C√≥digo en R

```r
# --- Preparaci√≥n: Cargar paquetes y datos ---
library(readr)
library(ggplot2)

# Cargamos los datos de la sesi√≥n anterior
ventas <- read_csv("datos_ventas.csv")
# ---------------------------------------------

# Ejemplo 1: Histograma y Densidad (Visual)
# ¬øC√≥mo se distribuyen los Precios?
ggplot(ventas, aes(x = Precio)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 10) +
  geom_density(color = "red", size = 1) +
  labs(title = "Distribuci√≥n de Precios")
# Visualmente, parece sesgado a la derecha (no perfectamente normal)
````

```r
# Ejemplo 2: Gr√°fico Q-Q (Visual - Recomendado)
# La funci√≥n base 'qqnorm' es excelente para esto.
qqnorm(ventas$Precio, main = "Q-Q Plot de Precios")
qqline(ventas$Precio, col = "red", lwd = 2) # A√±ade la l√≠nea de normalidad
# Interpretaci√≥n: Los puntos se desv√≠an de la l√≠nea roja
# en las colas (extremos), confirmando que no es normal.
```

```r
# Ejemplo 3: Test de Shapiro-Wilk (Estad√≠stico)
shapiro.test(ventas$Precio)

# Output:
#   Shapiro-Wilk normality test
# data:  ventas$Precio
# W = 0.95754, p-value = 0.0932
```

*Interpretaci√≥n del Ejemplo 3:* El **p-valor (0.0932)** es *mayor* que 0.05. Aunque visualmente vimos un sesgo, el test estad√≠stico *no tiene evidencia suficiente* para rechazar la normalidad. Para prop√≥sitos pr√°cticos, podr√≠amos considerarlo "suficientemente normal".

### ‚úèÔ∏è Ejercicios Propuestos

1.  **Pr√°ctica:** Genere un histograma y un gr√°fico de densidad (como en el Ejemplo 1) para la columna `Stock` del dataframe `ventas`. ¬øParece normal visualmente?
2.  **Laboratorio:** Genere un gr√°fico Q-Q (`qqnorm` y `qqline`) para la columna `Stock`. ¬øQu√© le dice este gr√°fico sobre la normalidad de los datos de Stock?
3.  **Evaluaci√≥n:** Ejecute el test `shapiro.test()` sobre la columna `Stock`. Interprete el resultado del p-valor. ¬øQu√© concluye sobre la normalidad del Stock?

### ‚úÖ Soluci√≥n a los Ejercicios

1.  **Pr√°ctica:**

    ```r
    # Asumiendo 'ggplot2' y 'ventas' cargados
    ggplot(ventas, aes(x = Stock)) +
      geom_histogram(aes(y = ..density..), fill = "orange", bins = 10) +
      geom_density(color = "blue", size = 1) +
      labs(title = "Distribuci√≥n de Stock")
    ```

    *Respuesta:* Visualmente, **no parece normal**. La mayor√≠a de los datos se concentra a la izquierda (stocks bajos) y tiene una "cola larga" hacia la derecha (sesgo positivo).

2.  **Laboratorio:**

    ```r
    qqnorm(ventas$Stock, main = "Q-Q Plot de Stock")
    qqline(ventas$Stock, col = "blue", lwd = 2)
    ```

    *Respuesta:* El gr√°fico Q-Q lo confirma. Los puntos forman una clara curva (una "banana") que se desv√≠a significativamente de la l√≠nea azul, especialmente en la cola superior (derecha). Esto es un signo claro de **no normalidad**.

3.  **Evaluaci√≥n:**

    ```r
    shapiro.test(ventas$Stock)

    # Output:
    #   Shapiro-Wilk normality test
    # data:  ventas$Stock
    # W = 0.84132, p-value = 8.13e-06 
    # (El p-valor es 0.00000813)
    ```

    *Interpretaci√≥n:* El **p-valor (8.13e-06)** es *extremadamente peque√±o* y mucho menor que 0.05.
    *Conclusi√≥n:* Rechazamos la hip√≥tesis nula ($H_0$). El test estad√≠stico confirma fuertemente lo que vimos visualmente: la variable `Stock` **no sigue una distribuci√≥n normal**.

-----

## 7.2 Correlaci√≥n entre Variables

### üí° Explicaci√≥n del Concepto

La **Correlaci√≥n** mide la **fuerza y direcci√≥n** de la **relaci√≥n lineal** entre dos variables num√©ricas.
El resultado es el **coeficiente de correlaci√≥n (r)**, un n√∫mero entre -1 y 1.

  * **r = 1 (Correlaci√≥n Positiva Perfecta):** Cuando A aumenta, B aumenta en proporci√≥n perfecta.
  * **r = 0.7 (Correlaci√≥n Positiva Fuerte):** Cuando A aumenta, B tiende a aumentar.
  * **r = 0 (Sin Correlaci√≥n Lineal):** No hay relaci√≥n lineal aparente.
  * **r = -0.8 (Correlaci√≥n Negativa Fuerte):** Cuando A aumenta, B tiende a disminuir.
  * **r = -1 (Correlaci√≥n Negativa Perfecta):** Cuando A aumenta, B disminuye en proporci√≥n perfecta.

> **¬°ADVERTENCIA FUNDAMENTAL: CORRELACI√ìN NO IMPLICA CAUSALIDAD\!**
> El cl√°sico ejemplo: las ventas de helados y los ataques de tiburones est√°n altamente correlacionados (ambos aumentan juntos). Pero los helados no causan ataques de tibur√≥n. Una tercera variable (la "temporada de verano") causa ambos.

**Tipos de Coeficientes:**

  * **Pearson (`method = "pearson"`):** El est√°ndar. Mide la relaci√≥n *lineal*. Es sensible a *outliers* (valores extremos). Requiere que los datos sean (m√°s o menos) normales.
  * **Spearman (`method = "spearman"`):** No param√©trico. Mide la relaci√≥n *monot√≥nica* (si A aumenta, B nunca disminuye, aunque no sea en l√≠nea recta). Se basa en los "rangos" (orden) de los datos. Es robusto a *outliers* y no requiere normalidad. **(Recomendado para nuestros datos de `Stock`, que no son normales)**.

**Matriz de Correlaci√≥n:**
Si tenemos muchas variables num√©ricas, podemos calcular la correlaci√≥n entre todas ellas a la vez con `cor()`.

### üíª Ejemplos de C√≥digo en R

```r
# Asumiendo que 'ventas' est√° cargado

# Ejemplo 1: Correlaci√≥n Pearson (lineal)
# ¬øQu√© relaci√≥n hay entre el Precio y el Stock?
cor(ventas$Precio, ventas$Stock, method = "pearson")

# Output: [1] -0.7303803
# Interpretaci√≥n: Hay una correlaci√≥n negativa FUERTE.
# A medida que el Precio sube, el Stock tiende a ser menor.
```

```r
# Ejemplo 2: Correlaci√≥n Spearman (robusta)
# Como 'Stock' no es normal, Spearman es m√°s apropiado.
cor(ventas$Precio, ventas$Stock, method = "spearman")

# Output: [1] -0.8354671
# Interpretaci√≥n: La correlaci√≥n negativa es a√∫n m√°s fuerte
# usando rangos. ¬°La relaci√≥n es muy robusta!
```

```r
# Ejemplo 3: Matriz de Correlaci√≥n
# Solo tenemos 2 variables num√©ricas, pero la sintaxis es esta:
# (Seleccionamos solo las columnas num√©ricas)
columnas_num <- ventas[, c("Precio", "Stock")]
matriz_cor <- cor(columnas_num)

print(matriz_cor)
#           Precio     Stock
# Precio  1.000000 -0.730380
# Stock  -0.730380  1.000000
```

### ‚úèÔ∏è Ejercicios Propuestos

1.  **Pr√°ctica:** Usando `dplyr`, cree una nueva columna en el dataframe `ventas` llamada `Valor_Total_Stock` (que sea `Precio * Stock`). (Pista: `ventas <- ventas %>% mutate(...)`).
2.  **Laboratorio:** Calcule la correlaci√≥n **Pearson** entre `Precio` y su nueva columna `Valor_Total_Stock`. Interprete el resultado.
3.  **Evaluaci√≥n:** Calcule la correlaci√≥n **Spearman** entre `Stock` y su nueva columna `Valor_Total_Stock`. Interprete el resultado.

### ‚úÖ Soluci√≥n a los Ejercicios

1.  **Pr√°ctica:**

    ```r
    library(dplyr)
    # Asumiendo que 'ventas' est√° cargado

    ventas <- ventas %>%
      mutate(Valor_Total_Stock = Precio * Stock)
      
    # Verificar
    head(ventas)
    #   Producto Precio Stock Valor_Total_Stock
    #   <chr>     <dbl> <dbl>             <dbl>
    # 1 Manzana    1.25   155              193.75
    # 2 Naranja    0.89   210              186.9 
    # ...
    ```

2.  **Laboratorio:**

    ```r
    # Asumiendo que 'ventas' tiene la nueva columna
    cor(ventas$Precio, ventas$Valor_Total_Stock, method = "pearson")

    # Output: [1] 0.2852601
    ```

    *Interpretaci√≥n:* Hay una correlaci√≥n positiva **d√©bil** (0.285). Sugiere que los productos m√°s caros *tienden* a tener un valor total de stock ligeramente mayor, pero la relaci√≥n no es fuerte.

3.  **Evaluaci√≥n:**

    ```r
    # Asumiendo que 'ventas' tiene la nueva columna
    cor(ventas$Stock, ventas$Valor_Total_Stock, method = "spearman")

    # Output: [1] 0.6974151
    ```

    *Interpretaci√≥n:* Hay una correlaci√≥n positiva **fuerte** (0.697) usando Spearman. Esto indica una relaci√≥n monot√≥nica clara: a mayor cantidad de stock, mayor es el valor total de ese stock. (Esto tiene m√°s sentido que la relaci√≥n con el precio).

-----

## 7.3 Visualizaci√≥n de Correlaci√≥n y Detecci√≥n de *Outliers*

### üí° Explicaci√≥n del Concepto

Nunca conf√≠e solo en el n√∫mero de correlaci√≥n. El **Cuarteto de Anscombe** es un famoso ejemplo de cuatro datasets que tienen *exactamente las mismas estad√≠sticas* (media, varianza, correlaci√≥n) pero que visualmente son radicalmente diferentes.

**¬°Siempre debe visualizar sus datos\!**

**Visualizaci√≥n de Correlaci√≥n:**
El mejor gr√°fico para ver una correlaci√≥n es el **gr√°fico de dispersi√≥n** (`geom_point()`).

  * Podemos a√±adir una l√≠nea de tendencia (`geom_smooth(method = "lm")`) para ver la relaci√≥n lineal m√°s claramente (lm = Linear Model).

**Detecci√≥n de Outliers (Valores At√≠picos)**
Un *outlier* es un punto de datos que est√° anormalmente lejos del resto.

  * Los *outliers* pueden **distorsionar gravemente** el coeficiente de correlaci√≥n de Pearson (porque "jalan" la l√≠nea de tendencia hacia ellos).
  * La mejor forma de detectarlos visualmente es con un **Boxplot** (`geom_boxplot()`). Los puntos que aparecen fuera de los "bigotes" (whiskers) del boxplot son candidatos a ser *outliers*.

### üíª Ejemplos de C√≥digo en R

```r
# Asumiendo 'ggplot2' y 'ventas' cargados

# Ejemplo 1: Gr√°fico de dispersi√≥n con l√≠nea de tendencia
# Visualizamos la correlaci√≥n negativa fuerte entre 'Precio' y 'Stock'
ggplot(ventas, aes(x = Precio, y = Stock)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # se=FALSE quita la sombra de error
  labs(title = "Relaci√≥n Precio vs. Stock")
```

```r
# Ejemplo 2: Visualizaci√≥n de una Matriz de Correlaci√≥n (Opcional)
# Si tenemos muchas variables, 'corrplot' es excelente
# install.packages("corrplot")
library(corrplot)

# Usamos la matriz calculada en la secci√≥n 7.2
columnas_num <- ventas[, c("Precio", "Stock", "Valor_Total_Stock")]
matriz_cor <- cor(columnas_num)
corrplot(matriz_cor, method = "number")
```

```r
# Ejemplo 3: Detecci√≥n de Outliers con Boxplot
# ¬øHay outliers en los Precios?
ggplot(ventas, aes(y = Precio)) +
  geom_boxplot() +
  labs(title = "Boxplot de Precios")
# (No parece haber outliers significativos en Precio)
```

### ‚úèÔ∏è Ejercicios Propuestos

1.  **Pr√°ctica:** Genere un **Boxplot** para la columna `Stock` (similar al Ejemplo 3). ¬øObserva alg√∫n *outlier*?
2.  **Laboratorio:** Genere el gr√°fico de dispersi√≥n (como en el Ejemplo 1) para `Precio` (eje X) vs `Stock` (eje Y). Esta vez, a√±ada `color = Producto` dentro del `aes()` para ver si la correlaci√≥n se mantiene para todos los productos.
3.  **Evaluaci√≥n (Conceptual):** Mirando el resultado del `shapiro.test()` para `Stock` (que no era normal) y la correlaci√≥n de Spearman (que fue m√°s fuerte que la de Pearson), ¬øPor qu√© cree que `Spearman` fue una mejor elecci√≥n que `Pearson` para analizar la relaci√≥n entre `Precio` y `Stock`?

### ‚úÖ Soluci√≥n a los Ejercicios

1.  **Pr√°ctica:**

    ```r
    ggplot(ventas, aes(y = Stock)) +
      geom_boxplot(fill = "red") +
      labs(title = "Boxplot de Stock")
    ```

    *Respuesta:* S√≠, se observa un *outlier* claro. Hay un punto (o varios muy juntos) muy por encima del "bigote" superior. Esto corresponde a los productos con mucho stock (ej. Pl√°tano, con 350 unidades) que se alejan del resto de los datos.

2.  **Laboratorio:**

    ```r
    ggplot(ventas, aes(x = Precio, y = Stock, color = Producto)) +
      geom_point(size = 2) + # Puntos un poco m√°s grandes
      geom_smooth(method = "lm", se = FALSE, aes(group = 1)) + # Una l√≠nea para todos
      labs(title = "Relaci√≥n Precio vs. Stock (por Producto)") +
      theme_minimal()
    ```

    *Interpretaci√≥n:* El gr√°fico muestra que la tendencia negativa (precio alto/stock bajo) se mantiene. Los productos caros (Fresa, Kiwi, Uva) est√°n en la esquina inferior derecha, mientras que los productos baratos (Pl√°tano, Naranja) est√°n en la esquina superior izquierda.

3.  **Evaluaci√≥n (Conceptual):**

      * **Pearson** mide relaciones *lineales* y es muy sensible a *outliers* y a la *no normalidad*.
      * **Spearman** mide relaciones *monot√≥nicas* (si una sube, la otra baja, aunque no sea en l√≠nea recta) y se basa en *rangos* (orden), no en los valores absolutos.
      * **Respuesta:** Spearman fue una mejor elecci√≥n porque (como vimos en el Ejercicio 1 de esta secci√≥n) la variable `Stock` tiene *outliers* y (como vimos en la Secci√≥n 7.1) **no es normal**. Los *outliers* (como el stock de 350) pueden "jalar" la l√≠nea de Pearson y distorsionar el resultado. Spearman ignora esta distancia extrema y solo ve que "Pl√°tano" tiene el *rango* de stock m√°s alto y uno of los *rangos* de precio m√°s bajos, capturando la tendencia monot√≥nica de forma m√°s robusta.
